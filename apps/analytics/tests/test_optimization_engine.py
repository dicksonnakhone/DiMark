"""Tests for decision engine, guardrails, and API endpoints (~17 tests)."""

from __future__ import annotations

import uuid
from datetime import date, datetime, timedelta, timezone

import pytest
from fastapi.testclient import TestClient
from sqlalchemy import create_engine
from sqlalchemy.orm import Session, sessionmaker
from sqlalchemy.pool import StaticPool

from app import models  # noqa: F401
from app.db import Base, get_db
from app.main import app
from app.models import (
    Campaign,
    ChannelSnapshot,
    DerivedKPI,
    OptimizationMethod,
    OptimizationProposal,
)
from app.services.optimization.engine import DecisionEngine, EngineResult
from app.services.optimization.guardrails import (
    check_budget_change_limit,
    check_cooldown,
    check_minimum_channel_floor,
    check_rate_limit,
)
from app.services.optimization.methods import build_default_registry


# ---------------------------------------------------------------------------
# Helpers
# ---------------------------------------------------------------------------


def _setup_db():
    engine = create_engine(
        "sqlite+pysqlite:///:memory:",
        connect_args={"check_same_thread": False},
        poolclass=StaticPool,
    )
    TestingSession = sessionmaker(bind=engine, autoflush=False, autocommit=False)
    Base.metadata.create_all(engine)
    return engine, TestingSession


def _make_campaign(db: Session, **kwargs) -> Campaign:
    defaults = dict(name="Test Campaign", objective="paid_conversions")
    defaults.update(kwargs)
    c = Campaign(**defaults)
    db.add(c)
    db.commit()
    db.refresh(c)
    return c


def _add_snapshot(
    db: Session,
    campaign_id,
    channel: str = "meta",
    spend: float = 1000.0,
    impressions: int = 100_000,
    clicks: int = 1000,
    conversions: int = 50,
    revenue: float = 5000.0,
    window_start: date | None = None,
    window_end: date | None = None,
):
    snap = ChannelSnapshot(
        campaign_id=campaign_id,
        channel=channel,
        spend=spend,
        impressions=impressions,
        clicks=clicks,
        conversions=conversions,
        revenue=revenue,
        window_start=window_start,
        window_end=window_end,
    )
    db.add(snap)
    db.commit()
    return snap


# ---------------------------------------------------------------------------
# Guardrail tests
# ---------------------------------------------------------------------------


class TestGuardrails:
    def test_budget_change_within_limit(self):
        current = {"meta": 1000.0, "google": 500.0}
        proposed = {"meta": 900.0, "google": 600.0}
        result = check_budget_change_limit(current, proposed, max_change_pct=0.20)
        assert result.passed is True

    def test_budget_change_exceeds_limit(self):
        current = {"meta": 1000.0, "google": 500.0}
        proposed = {"meta": 500.0, "google": 1000.0}  # 50 % change
        result = check_budget_change_limit(current, proposed, max_change_pct=0.20)
        assert result.passed is False
        assert "budget change" in result.message.lower()

    def test_rate_limit_within(self):
        now = datetime.now(tz=timezone.utc)
        recent = [now - timedelta(minutes=30)]
        result = check_rate_limit(recent, max_per_hour=3)
        assert result.passed is True

    def test_rate_limit_exceeded(self):
        now = datetime.now(tz=timezone.utc)
        recent = [
            now - timedelta(minutes=10),
            now - timedelta(minutes=20),
            now - timedelta(minutes=30),
        ]
        result = check_rate_limit(recent, max_per_hour=3)
        assert result.passed is False

    def test_cooldown_elapsed(self):
        last = datetime.now(tz=timezone.utc) - timedelta(hours=2)
        result = check_cooldown("cpa_spike", last, cooldown_minutes=60)
        assert result.passed is True

    def test_cooldown_active(self):
        last = datetime.now(tz=timezone.utc) - timedelta(minutes=30)
        result = check_cooldown("cpa_spike", last, cooldown_minutes=60)
        assert result.passed is False

    def test_cooldown_never_fired(self):
        result = check_cooldown("cpa_spike", None, cooldown_minutes=60)
        assert result.passed is True

    def test_channel_floor_ok(self):
        proposed = {"meta": 800.0, "google": 200.0}
        result = check_minimum_channel_floor(proposed, min_floor_pct=0.05)
        assert result.passed is True

    def test_channel_floor_violation(self):
        proposed = {"meta": 990.0, "google": 10.0}  # google = 1 % < 5 %
        result = check_minimum_channel_floor(proposed, min_floor_pct=0.05)
        assert result.passed is False


# ---------------------------------------------------------------------------
# Engine tests
# ---------------------------------------------------------------------------


class TestDecisionEngine:
    def test_no_campaign(self):
        _, Session = _setup_db()
        db = Session()
        registry = build_default_registry()
        engine = DecisionEngine(registry)

        result = engine.run(db, str(uuid.uuid4()))
        assert result.success is False
        assert "not found" in result.errors[0].lower()
        db.close()

    def test_no_snapshots(self):
        _, Session = _setup_db()
        db = Session()
        campaign = _make_campaign(db)
        registry = build_default_registry()
        engine = DecisionEngine(registry)

        result = engine.run(db, str(campaign.id))
        assert result.success is False
        assert "snapshot" in result.errors[0].lower()
        db.close()

    def test_creates_proposals_when_issues_detected(self):
        _, Session = _setup_db()
        db = Session()
        campaign = _make_campaign(db)

        # Add snapshots with big efficiency spread between channels
        _add_snapshot(
            db, campaign.id, channel="meta",
            spend=3000, impressions=300_000, clicks=3000,
            conversions=200, revenue=9000,
            window_start=date(2025, 1, 1), window_end=date(2025, 1, 7),
        )
        _add_snapshot(
            db, campaign.id, channel="google",
            spend=2000, impressions=200_000, clicks=500,
            conversions=10, revenue=500,
            window_start=date(2025, 1, 1), window_end=date(2025, 1, 7),
        )

        registry = build_default_registry()
        engine = DecisionEngine(registry)
        result = engine.run(db, str(campaign.id))

        assert result.success is True
        # At minimum we should have processed methods
        assert result.method_evaluations >= 0
        db.close()

    def test_auto_approve_high_confidence(self):
        """When confidence >= threshold, proposals should be auto-approved."""
        _, Session = _setup_db()
        db = Session()
        campaign = _make_campaign(db)

        # Seed many snapshots to avoid confidence penalty for sparse data
        for i in range(10):
            _add_snapshot(
                db, campaign.id, channel="meta",
                spend=3000, impressions=300_000, clicks=3000,
                conversions=200, revenue=9000,
                window_start=date(2025, 1, 1) + timedelta(weeks=i),
                window_end=date(2025, 1, 7) + timedelta(weeks=i),
            )
            _add_snapshot(
                db, campaign.id, channel="google",
                spend=2000, impressions=200_000, clicks=200,
                conversions=5, revenue=250,
                window_start=date(2025, 1, 1) + timedelta(weeks=i),
                window_end=date(2025, 1, 7) + timedelta(weeks=i),
            )

        registry = build_default_registry()
        engine = DecisionEngine(registry)
        result = engine.run(db, str(campaign.id))

        assert result.success is True
        # Check DB for proposal statuses
        proposals = db.query(OptimizationProposal).filter_by(
            campaign_id=campaign.id,
        ).all()
        # Some may be auto_approved, some pending depending on confidence
        statuses = {p.status for p in proposals}
        assert statuses.issubset({"auto_approved", "pending"})
        db.close()

    def test_queues_low_confidence(self):
        """With sparse data, proposals should be queued (pending)."""
        _, Session = _setup_db()
        db = Session()
        campaign = _make_campaign(db)

        # Very minimal data â†’ confidence will be penalized
        _add_snapshot(
            db, campaign.id, channel="meta",
            spend=5000, impressions=500_000, clicks=5000,
            conversions=250, revenue=12500,
            window_start=date(2025, 1, 1), window_end=date(2025, 1, 7),
        )
        _add_snapshot(
            db, campaign.id, channel="google",
            spend=1000, impressions=100_000, clicks=100,
            conversions=2, revenue=100,
            window_start=date(2025, 1, 1), window_end=date(2025, 1, 7),
        )

        registry = build_default_registry()
        engine = DecisionEngine(registry)
        result = engine.run(db, str(campaign.id))

        assert result.success is True
        # With only 2 snapshots, confidence should be penalized
        # proposals that fire should have reduced confidence
        db.close()


# ---------------------------------------------------------------------------
# API tests
# ---------------------------------------------------------------------------


class TestOptimizationAPI:
    @staticmethod
    def _setup_client():
        engine, TestingSession = _setup_db()

        def override_get_db():
            db = TestingSession()
            try:
                yield db
            finally:
                db.close()

        app.dependency_overrides[get_db] = override_get_db
        client = TestClient(app)
        return client, TestingSession

    def test_run_optimization_no_campaign(self):
        client, _ = self._setup_client()
        resp = client.post(f"/api/optimization/campaigns/{uuid.uuid4()}/run")
        assert resp.status_code == 404

    def test_run_optimization_success(self):
        client, Session = self._setup_client()
        db = Session()
        campaign = _make_campaign(db)
        campaign_id = campaign.id  # Capture before closing session
        _add_snapshot(db, campaign_id, channel="meta", spend=1000, clicks=100, conversions=10)
        _add_snapshot(db, campaign_id, channel="google", spend=500, clicks=50, conversions=5)
        db.close()

        resp = client.post(f"/api/optimization/campaigns/{campaign_id}/run")
        assert resp.status_code == 200
        data = resp.json()
        assert data["success"] is True

    def test_get_metrics(self):
        client, Session = self._setup_client()
        db = Session()
        campaign = _make_campaign(db)
        db.close()

        resp = client.get(f"/api/optimization/campaigns/{campaign.id}/metrics")
        assert resp.status_code == 200
        data = resp.json()
        assert data["campaign_id"] == str(campaign.id)

    def test_list_proposals_empty(self):
        client, Session = self._setup_client()
        db = Session()
        campaign = _make_campaign(db)
        db.close()

        resp = client.get(f"/api/optimization/campaigns/{campaign.id}/proposals")
        assert resp.status_code == 200
        assert resp.json() == []

    def test_approve_reject_proposal(self):
        client, Session = self._setup_client()
        db = Session()
        campaign = _make_campaign(db)

        # Create a method and proposal manually
        method = OptimizationMethod(
            name="test_method",
            description="A test method",
            method_type="reactive",
            trigger_conditions={},
            config_json={},
            is_active=True,
            cooldown_minutes=60,
            stats_json={},
        )
        db.add(method)
        db.flush()

        proposal = OptimizationProposal(
            campaign_id=campaign.id,
            method_id=method.id,
            status="pending",
            confidence=0.75,
            priority=5,
            action_type="budget_reallocation",
            action_payload={"test": True},
            reasoning="Test proposal",
            trigger_data_json={},
            guardrail_checks_json={},
        )
        db.add(proposal)
        db.commit()
        db.refresh(proposal)
        proposal_id = proposal.id
        db.close()

        # Approve
        resp = client.post(
            f"/api/optimization/proposals/{proposal_id}/approve",
            json={"action": "approve", "approved_by": "tester"},
        )
        assert resp.status_code == 200
        data = resp.json()
        assert data["status"] == "approved"
        assert data["approved_by"] == "tester"

    def test_list_methods_empty(self):
        client, _ = self._setup_client()
        resp = client.get("/api/optimization/methods")
        assert resp.status_code == 200
        assert resp.json() == []

    def test_update_method(self):
        client, Session = self._setup_client()
        db = Session()
        method = OptimizationMethod(
            name="test_method",
            description="A test method",
            method_type="reactive",
            trigger_conditions={},
            config_json={},
            is_active=True,
            cooldown_minutes=60,
            stats_json={},
        )
        db.add(method)
        db.commit()
        db.refresh(method)
        method_id = method.id
        db.close()

        resp = client.patch(
            f"/api/optimization/methods/{method_id}",
            json={"is_active": False, "cooldown_minutes": 120},
        )
        assert resp.status_code == 200
        data = resp.json()
        assert data["is_active"] is False
        assert data["cooldown_minutes"] == 120
